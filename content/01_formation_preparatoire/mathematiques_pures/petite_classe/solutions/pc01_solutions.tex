% =========================================================
\section*{Algèbre Linéaire}
\addcontentsline{toc}{section}{Algèbre Linéaire}
% =========================================================

\subsection*{Exercice 1}

Comme $\ker f$ est un sous-espace vectoriel de $E$, on peut choisir une base de $\ker f$: $B_{\ker} = (v_1, v_2, \cdots, v_k)$. Complétons la base, avec des vecteurs $u_i$ afin de construire une base de $E$:

\[B_E = (v_1, v_2, \cdots, v_k, u_{k+1}, \cdots, u_n)\]

Maintenant, voyez que pour chaque vecteur $w$ appartenant à l'image de $f$, on peut l'écrire comme:

\[w = f(v_E) = f\left(\lambda_1 v_1 + \lambda_2 v_2 + \cdots + \lambda_k v_k + \lambda_{k+1} u_{k+1} + \cdots + \lambda_n u_n\right)\]

Mais, $f(v_i) = 0,\ i \le k$, alors:

\[\begin{split}
	w &= f\left(\lambda_{k+1} u_{k+1} + \cdots + \lambda_n u_n\right) \\
	 &= \lambda_{k+1} f(u_{k+1}) + \cdots + \lambda_nf(u_n)
	\end{split}\]

Donc on voie qu'ils sont combinaisons linéaires du ensemble $A = (f(u_{k+1}), \cdots, f(u_n)$ (on dit que $A$ engendre $\imag f$). Mais $A$ est aussi linéairement indépendant, car $w = 0$ si et seulement si:

\[\lambda_{k+1} u_{k+1} + \cdots + \lambda_n u_n = v' \in \ker f\]

Mais comme $v' \in \ker f$ donc $v' = \alpha_1 v_1 + \cdots + \alpha_kv_k$, donc:

\[-\alpha_1 v_1 - \cdots - \alpha_k v_k + \lambda_{k+1} u_{k+1} + \cdots + \lambda_n u_n = 0\]

Mais, comme $B_E$ est une base de $E$, cela n'arrive que si $\alpha_1 = \cdots = \alpha_k = \lambda_{k+1} = \cdots = \lambda_n = 0$.

Alors $A$ est une base de $\imag f$, donc:

\[\dim \ker f + \dim A = \dim E\]

\[\dim \ker f + \dim \imag f = \dim E\ \QED\]



\subsection*{Exercice 2}

On doit demontrer que $1 \implies 2 \implies 3 \implies 1$ (ou quelque autre combinaison cyclique):


\begin{enumerate}
	\item $1 \implies 2$
	
	C'est direct que $\imag f^2 \subset \imag f$, donc on doit montrer que $\imag f \subset \imag f^2$.
	
	Soit $v \in \imag f$ on a que: $v = f(u)$. Comme l'espace $V = \ker f \oplus \imag f$, on peut dire que $u = x + f(w)$ où $x \in \ker f $ et $w \notin \ker f$. $v = f(u) = f(x + f(w)) = f(f(w)) \in \imag f^2$. Donc $\imag f \subset \imag f^2\ \QED$.
	
	
	\item $2 \implies 3 $
	
	C'est direct que $\ker f \subset \ker f^2$.
	
	Pour le théorème du rang (démontré dans l'exercice 1):
	
	\[\begin{cases}
		\dim \imag f + \dim \ker f = n \\
		\dim \imag f^2 + \dim \ker f^2 = n 
	\end{cases}\]
	
	Mais comme $\dim \imag f^2 = \dim \imag f$ alors $\dim \ker f = \dim \ker f^2$, donc:
	
	\[\ker f = \ker f^2\ \QED\] 
	
	
	\item $3 \implies 1$
	
	Si l'on montre que $\ker (f) \cap \imag f = \{0\}$, par le théorème du rang, on montre aussi que $\ker f + \imag f = \ker(f) \oplus \imag f = V$.
	
	En supposant $v \in \ker f$ et $v \in \imag f$, on a que
	
	
	\[\begin{cases}
		f(v) = 0 \\
		v = f(u)\ |\ $u \in V$
	\end{cases}\].
	
	\[\implies f(f(u)) = 0\]
	
	Alors on voit que $u \in \ker f^2 = \ker f\ \implies f(u) = 0$, donc $v = 0$. Ainsi $\ker (f) \cap \imag f = \{0\}$ $\blacksquare$.
	
\end{enumerate}



\subsection*{Exercice 3}


Par le théorème du rang: $u_n = v_n$, de façon qu'on ne doit analyser qu'une suite. Pour cela, on utilisera l'application suivante:

\[g: \imag f^n \to \imag f^{n+1}\ |\ g(x) = f(x)\]

Voyez que $g(x)$ est bien définie puisque si $v \in \imag f^n$ ($v = f^n(u)\ |\ u \in E$), donc $g(v) = f^{n+1}(u) \in \imag f^{n+1}$. Voyez aussi que $\ker g = \ker f \cap \imag f^{n}$, car son noyau sont tous les éléments du noyau de $f$ qui font partie de $\imag f^{n}$.

Alors:

\[\dim \ker g + \dim \imag g = \dim \imag f^n\]

Comme $\dim \imag g = \dim \imag f^{n+1}$:

\[\dim (\ker f \cap \imag f^{n}) = \dim \imag f^n - \dim \imag f^{n+1} = u_n\]

De cette façon, comme $u_n$ est la dimension d'un espace vectoriel, il sera $u_n \ge 0$. Pour montrer qu'il est décroissant, il suffit de voir que $\ker f \cap \imag f^{n+1} \subset \ker f \cap \imag f^n$, donc la dimension ne peut qu'être constante ou décroître. Donc, étant une suite monotone et bornée, elle converge.

Or, observant la somme suivante:

\[U_k = \sum_{i=1}^k u_i = \dim \imag f - \dim \imag f^{k+1}\]

Et considérant que $\dim \imag f^{k+1}$ aussi converge (par la même raison d'être une suite monotone bornée), on peut conclure que $U_k$ converge aussi. Donc il faut que $u_n \to 0$ car s'il n'était pas le cas, la somme divergerait.



\subsection*{Exercice 4}

\begin{enumerate}
	\item $P(x) \in \ker \varphi \implies P(x+1) = P(x)$.  Si c'est le cas, on peut analyser le polynôme dans un sous-ensemble du domaine $A = \{a+ib\ |\ a \in [0, 1[,\ b \in \R\}$ car le reste du domaine pourrait être défini en fonction de cette partie\footnote{Parce que $P(x) = P\left( x - \lfloor x \rfloor \right)$, par induction.}.
	
	S'il y avait quelque racine $r$ de $P(x)\ | r \in A$, donc le polynôme aurait un nombre infini de racines ($r + n\ \forall n \in \Z$ seraient racines), ce qui n'est pas possible, sauf si $P(x) = 0$. Et si $P(x)$ n'a pas de racines en $A$ $\implies$ $P(x)$ n'a pas de racines dans tout son domaine $\implies$ $P(x)$ est constante.
	
	Donc $\ker \varphi = \{P(x) = c\ |\ c \in \C\}$.
	
	
	\item On peut choisir une base de $\C_n[X]:\ (1, x, x^2, ..., x^n)$ pour voir que: 
	
	\[\begin{cases}
		\varphi(1) = 0 \\
		\varphi(x) = 1 \\
		\varphi(x^2) = 2x + 1 \\
		\vdots \\
		\varphi(x^n) = \binom{n}{1} x^{n-1} + \binom{n}{2} x^{n-2} + \cdots + \binom{n}{n-1}x + \binom{n}{n}
	\end{cases}\]
	
	Voyez que $(\varphi(x), \phi(x^2), \cdots, \varphi(x^n))$ est linéairement indépendante, car chaque terme $\varphi(x^i)$ contrôle le coefficient de $x^{i-1}$. Et, d'ailleurs:
	
	\[\imag \varphi = \{P(X) \in \C_{n-1}[X]\}\]
	
\end{enumerate}




\subsection*{Exercice 5}

\begin{enumerate} 
	\item $(a) \implies (b)$
	
	Si $f = 0$ ou $g = 0$ c'est évident, donc soit $f != 0$ et alors $g != 0$ par $(a)$.
	
	Comme $\dim \ker f + \dim \imag f = n$ et $\dim \imag f = 1$, puisque $\imag f \subset \R$ et $\imag f != \{0\}$, on a que $\dim \ker f = \dim \ker g = n-1$. Alors, on peut construire une base $B$ de $\R^n$ telle que $B = (u_1, \cdots, u_{n-1}, v_n)$ où $u_i \in \ker f$ et $f(v_n), g(v_n) \neq 0$. 
	
	D'ailleurs soit $x \in \R_n\ |\ x = \lambda_1 u_1 + \cdots + \lambda_n v_n$:
	
	\[\begin{cases}
		f(x) = \lambda_n f(v_n) \\
		g(x) = \lambda_n g(v_n)
	\end{cases}\] 
	
	\[\implies f(x) = \frac{f(v_n)}{g(v_n)} g(x)\]
	
	\item $(b) \implies (a)$
	
	Si $v \in \ker f$: $f(v) = \lambda g(v) = 0 \implies g(v) = 0 \implies v \in \ker g$, donc $\ker f \subset \ker g$. Si $u \in \ker g$: $g(u) = \dfrac{1}{\lambda} f(u) = 0\ \implies f(u) = 0 \implies u \in \ker f$, donc $\ker g \subset \ker f$.
	
	Alors $\ker f = \ker g$.
	
\end{enumerate}






\subsection{Exercice 6}


\begin{enumerate}
	\item $P(x) = \det(M - x\cdot \id)$
	
	\[\det\begin{pmatrix}
		1 - x && 0 && 1 \\
		-1 && 2-x && 1 \\
		2-m && m-2 && m-x \\
	\end{pmatrix} = (1-x)(2-x)(m-x)\]
	
	Le polynôme minimal dépendra si $m = 1$ ou $m=2$ et si pour chaque un de ces valeurs la dimension de l'espace propre sera unique ou non, alors on verra après.
	
	
	\item Si $m \in \R \backslash \{1, 2\}$, la matrice est diagonalisable, puisqu'elle a 3 valeurs propres distincts. Maintenant pour:
	
	\begin{enumerate}
		\item $m = 1$
		
		\[A(1) = \begin{pmatrix}
			1 && 0 && 1 \\
			-1 && 2 && 1 \\
			1 && -1 && 1 \\
		\end{pmatrix}\]
		
		et, en calculant les vecteurs propres:
		
		\[Av = v = \begin{bmatrix}
			a \\
			b \\
			c \\
		\end{bmatrix}\]
		
		on arrive à la condition que $a = b$ et $c = 0$, de manière que $v = \lambda [1, 1, 0]$. Comme il n'y a que 1 dégrée de liberté, on arrive que $\dim E(1) < m(1) = $ la multiplicité polynômial de la racine. Donc il faut que $m \neq 1$.
		
		
		\item $m = 2$
		
		\[A(2) = \begin{pmatrix}
			1 && 0 && 1 \\
			-1 && 2 && 1 \\
			0 && 0 && 2 
		\end{pmatrix}\]
		
		En calculant les vecteurs propres on arrive que $a = c$ sans aucune restriction sur $b$, donc on a 2 dégrées de liberté pour choisir un vecteur propre: $v = \lambda_1 [1, 0, 1] + \lambda_2 [0, 1, 0]$. Donc même si $m = 2$ $A(m)$ est diagonalisable.
		
		
	\end{enumerate}
	
	De cette manière la condition cherchée est $m \in \R \backslash \{1\}$.
	
	
	\item Pour $m \neq 2$, les valeurs propres sont $1, 2, m$. Comme on a vu avant $v(\lambda = 1) = [1, 1, 0]$. Pour $v(\lambda = 2)$ on trouve qu'un vecteur propre associé est $[1, 0, 1]$. Pour $v(\lambda = m)$ on fait le même processus pour trouver une solution égale à $[1, 1, m-1]$:
	
	
	\[D = \diagonal{1 2 m}\]
	
	et 
	
	\[P = \begin{pmatrix}
		1 && 1 && 1 \\
		1 && 0 && 1 \\
		0 && 1 && m-1 \\
	\end{pmatrix}\]
	
	
	Pour $m = 2$, on va utiliser les résultats déjà montrés:
	
	\[D = \diagonal{1 2 2}\]
	
	et
	
	\[P = \begin{pmatrix}
		1 && 1 && 0 \\
		1 && 0 && 1 \\
		0 && 1 && 0 \\
	\end{pmatrix}\]
	
	
	\subsection*{Exercice 7}
	
	En calculant son polynôme caractéristique
	
	\[\det (M(a) - x\cdot \id) = \begin{pmatrix}
		2 - x && 0 && 1 - a \\
		-1 && 1 - x && a - 1 \\
		a-1 && 0 && 2a - x 
	\end{pmatrix} = (1-x)(x - a - 1)^2\]
	
	Donc on a les valeurs propres $\lambda_1 = 1$ et $\lambda_2 = a + 1$, où $\lambda_2$ a multiplicité arithmétique $2$, donc on a besoin de déterminer si la multiplicité géométrique est aussi $2$, donc on cherchera les vecteurs $v = [x\ y\ z]\ | M(a)v = (a+1)v$:
	
	\[\begin{bmatrix}
		(a+1)x \\ (a+1)y \\ (a+1)z
	\end{bmatrix} = \begin{pmatrix}
	2 && 0 && 1-a \\
	-1 && 1 && a-1 \\
	a-1 && 0 && 2a
	\end{pmatrix}\begin{bmatrix}
	x \\ y \\z
	\end{bmatrix}\]
	
	On trouve que $(a-1)(x+z) = a(x+y) = 0$. Alors on va considérer les cas $a = 0, a = 1$ et $a \in \R\backslash\{0, 1\}$:
	
	\begin{enumerate}
		\item $a = 1$
		
		En ce cas $\lambda_1 \neq \lambda_2$ et on a que $x + y = 0$, de façon que le sous-espace propre de $\lambda_2$ a dimension\footnote{On peut choisir une base $B = ([1, -1, 0], [0, 0, 1])$ par exemple.} 2, qui, avec le sous-espace propre associé à $\lambda_1$, engendrent $\R^3$. \textbf{Donc la matrice est diagonalisable}.
		
		
		\item $a = 0$
		
		En ce cas $\lambda_1 = \lambda_2 = 1$, mais on la restriction pour les vecteurs propres: $x + z = 0$. Mais cette restriction permet un sous-espace de dimension 2 qui n'est pas capable d'engendrer $\R^3$. \textbf{Donc la matrice n'est pas diagonalisable}.
		
		
		\item $a \neq 0$ et $a \neq 1$
		
		En ce cas $\lambda_1 \neq \lambda_2$, et on a que $x + y = x + z = 0$. Mais cette restriction permet un sous-espace de dimension 1, qui même avec le sous-espace propre associé à $\lambda_1$ n'engendrent pas le $\R^3$. \textbf{Donc la matrice n'est pas diagonalisable}.
	\end{enumerate}
	
	
	Dans le cas $a = 1$:
	
	\[P = \begin{pmatrix}
		0 && 1 && 0\\
		1 && -1 &&0 \\
		0 && 0 && 1
	\end{pmatrix}\]
	
	et 
	
	\[D = \diagonal{1 2 2}\]
	
	
	\subsection*{Exercice 8}
	
	\begin{enumerate}
		\item D'abord on a que $B = (\varphi_0, \cdots, \varphi_n)$ et, comme $B$ a $n+1 = \dim E^*$ éléments, si l'on preuve que $B$ est linéairement indépendante, on montre qu'elle est une base de $E^*$.
		
		Alors, supposons que $\lambda_0 \varphi_0 + \cdots + \lambda_n \varphi_n = 0$, donc on a que:
		
		\[\left(\sum_{i=0}^n\lambda_i \varphi_i\right)(P) = 0\ (\forall P \in \R_n[X])\ \implies \sum_{i  = 0}^n \lambda_i P(a_i) = 0\ (\forall P \in \R_n[X])\] 
		
		Comme $a_i \neq a_j$ $\forall i \neq j$, on peut choisir des différents polynômes $Q_k(x)$ tels que $a_i$ sont racines de $Q_k$ sauf $a_k$ où $Q_k(a_k) = 1$:
		
		\[Q_k(x) = \alpha_k \cdot (x - a_0)\cdots(x- a_{k-1})(x-a_{k+1})\cdots (x-a_n)\]
		
		où $\alpha_k \in \R$ est la constant qui rend $Q_k(a_k) = 1$.
		
		De cette façon on montre que chaque $\lambda_k = 0$. Donc $B$ est base de $E^*$.
		
		\item Pour la définition de l'espace dual: 
		\[\phi_i(P_j) = P_j(a_i) = \delta_{i, j} = \begin{cases}
			1,\ \text{ si } i = j\\
			0,\ \text{ si } i \neq j
		\end{cases}\]
		
		On peut voir que les polynômes $Q_k(x)$ présentés dans l'item antérieur satisfont ces conditions. Pour montrer que $B = (Q_0, \cdots, Q_n)$ forme une base de $E$, il suffit de montrer que cet ensemble est linéairement indépendant, étant donné que $B$ contient $n+1 = \dim E$ éléments. Mais comme seulement $Q_i(x)$ n'est pas nul en $x = a_i$, il est claire que $B$ est L.I.		
		
	\end{enumerate}
	
	
	
\end{enumerate}




