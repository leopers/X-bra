% =========================================================
\section*{Algèbre Linéaire}
% =========================================================

\subsection*{Exercice 1}

Comme $\ker f$ est un sous-espace vectoriel de $E$, on peut choisir une base de $\ker f$: $B_{\ker} = (v_1, v_2, \cdots, v_k)$. Complétons la base, avec des vecteurs $u_i$ afin de construire une base de $E$:

\[B_E = (v_1, v_2, \cdots, v_k, u_{k+1}, \cdots, u_n)\]

Maintenant, voyez que pour chaque vecteur $w$ appartenant à l'image de $f$, on peut l'écrire comme:

\[w = f(v_E) = f\left(\lambda_1 v_1 + \lambda_2 v_2 + \cdots + \lambda_k v_k + \lambda_{k+1} u_{k+1} + \cdots + \lambda_n u_n\right)\]

Mais, $f(v_i) = 0,\ i \le k$, alors:

\[\begin{split}
	w &= f\left(\lambda_{k+1} u_{k+1} + \cdots + \lambda_n u_n\right) \\
	 &= \lambda_{k+1} f(u_{k+1}) + \cdots + \lambda_nf(u_n)
	\end{split}\]

Donc on voie qu'ils sont combinaisons linéaires du ensemble $A = (f(u_{k+1}), \cdots, f(u_n)$ (on dit que $A$ engendre $\imag f$). Mais $A$ est aussi linéairement indépendant, car $w = 0$ si et seulement si:

\[\lambda_{k+1} u_{k+1} + \cdots + \lambda_n u_n = v' \in \ker f\]

Mais comme $v' \in \ker f$ donc $v' = \alpha_1 v_1 + \cdots + \alpha_kv_k$, donc:

\[-\alpha_1 v_1 - \cdots - \alpha_k v_k + \lambda_{k+1} u_{k+1} + \cdots + \lambda_n u_n = 0\]

Mais, comme $B_E$ est une base de $E$, cela n'arrive que si $\alpha_1 = \cdots = \alpha_k = \lambda_{k+1} = \cdots = \lambda_n = 0$.

Alors $A$ est une base de $\imag f$, donc:

\[\dim \ker f + \dim A = \dim E\]

\[\dim \ker f + \dim \imag f = \dim E\ \QED\]



\subsection*{Exercice 2}

On doit demontrer que $1 \implies 2 \implies 3 \implies 1$ (ou quelque autre combinaison cyclique):


\begin{enumerate}
	\item $1 \implies 2$
	
	C'est direct que $\imag f^2 \subset \imag f$, donc on doit montrer que $\imag f \subset \imag f^2$.
	
	Soit $v \in \imag f$ on a que: $v = f(u)$. Comme l'espace $V = \ker f \oplus \imag f$, on peut dire que $u = x + f(w)$ où $x \in \ker f $ et $w \notin \ker f$. $v = f(u) = f(x + f(w)) = f(f(w)) \in \imag f^2$. Donc $\imag f \subset \imag f^2\ \QED$.
	
	
	\item $2 \implies 3 $
	
	C'est direct que $\ker f \subset \ker f^2$.
	
	Pour le théorème du rang (démontré dans l'exercice 1):
	
	\[\begin{cases}
		\dim \imag f + \dim \ker f = n \\
		\dim \imag f^2 + \dim \ker f^2 = n 
	\end{cases}\]
	
	Mais comme $\dim \imag f^2 = \dim \imag f$ alors $\dim \ker f = \dim \ker f^2$, donc:
	
	\[\ker f = \ker f^2\ \QED\] 
	
	
	\item $3 \implies 1$
	
	Si l'on montre que $\ker (f) \cap \imag f = \{0\}$, par le théorème du rang, on montre aussi que $\ker f + \imag f = \ker(f) \oplus \imag f = V$.
	
	En supposant $v \in \ker f$ et $v \in \imag f$, on a que
	
	
	\[\begin{cases}
		f(v) = 0 \\
		v = f(u)\ |\ $u \in V$
	\end{cases}\].
	
	\[\implies f(f(u)) = 0\]
	
	Alors on voit que $u \in \ker f^2 = \ker f\ \implies f(u) = 0$, donc $v = 0$. Ainsi $\ker (f) \cap \imag f = \{0\}$ $\blacksquare$.
	
\end{enumerate}



\subsection*{Exercice 3}


Par le théorème du rang: $u_n = v_n$, de façon qu'on ne doit analyser qu'une suite. Pour cela, on utilisera l'application suivante:

\[g: \imag f^n \to \imag f^{n+1}\ |\ g(x) = f(x)\]

Voyez que $g(x)$ est bien définie puisque si $v \in \imag f^n$ ($v = f^n(u)\ |\ u \in E$), donc $g(v) = f^{n+1}(u) \in \imag f^{n+1}$. Voyez aussi que $\ker g = \ker f \cap \imag f^{n}$, car son noyau sont tous les éléments du noyau de $f$ qui font partie de $\imag f^{n}$.

Alors:

\[\dim \ker g + \dim \imag g = \dim \imag f^n\]

Comme $\dim \imag g = \dim \imag f^{n+1}$:

\[\dim (\ker f \cap \imag f^{n}) = \dim \imag f^n - \dim \imag f^{n+1} = u_n\]

De cette façon, comme $u_n$ est la dimension d'un espace vectoriel, il sera $u_n \ge 0$. Pour montrer qu'il est décroissant, il suffit de voir que $\ker f \cap \imag f^{n+1} \subset \ker f \cap \imag f^n$, donc la dimension ne peut qu'être constante ou décroître. Donc, étant une suite monotone et bornée, elle converge.

Or, observant la somme suivante:

\[U_k = \sum_{i=1}^k u_i = \dim \imag f - \dim \imag f^{k+1}\]

Et considérant que $\dim \imag f^{k+1}$ aussi converge (par la même raison d'être une suite monotone bornée), on peut conclure que $U_k$ converge aussi. Donc il faut que $u_n \to 0$ car s'il n'était pas le cas, la somme divergerait.



\subsection*{Exercice 4}

\begin{enumerate}
	\item $P(x) \in \ker \varphi \implies P(x+1) = P(x)$.  Si c'est le cas, on peut analyser le polynôme dans un sous-ensemble du domaine $A = \{a+ib\ |\ a \in [0, 1[,\ b \in \R\}$ car le reste du domaine pourrait être défini en fonction de cette partie\footnote{Parce que $P(x) = P\left( x - \lfloor x \rfloor \right)$, par induction.}.
	
	S'il y avait quelque racine $r$ de $P(x)\ | r \in A$, donc le polynôme aurait un nombre infini de racines ($r + n\ \forall n \in \Z$ seraient racines), ce qui n'est pas possible, sauf si $P(x) = 0$. Et si $P(x)$ n'a pas de racines en $A$ $\implies$ $P(x)$ n'a pas de racines dans tout son domaine $\implies$ $P(x)$ est constante.
	
	Donc $\ker \varphi = \{P(x) = c\ |\ c \in \C\}$.
	
	
	\item On peut choisir une base de $\C_n[X]:\ (1, x, x^2, ..., x^n)$ pour voir que: 
	
	\[\begin{cases}
		\varphi(1) = 0 \\
		\varphi(x) = 1 \\
		\varphi(x^2) = 2x + 1 \\
		\vdots \\
		\varphi(x^n) = \binom{n}{1} x^{n-1} + \binom{n}{2} x^{n-2} + \cdots + \binom{n}{n-1}x + \binom{n}{n}
	\end{cases}\]
	
	Voyez que $(\varphi(x), \phi(x^2), \cdots, \varphi(x^n))$ est linéairement indépendante, car chaque terme $\varphi(x^i)$ contrôle le coefficient de $x^{i-1}$. Et, d'ailleurs:
	
	\[\imag \varphi = \{P(X) \in \C_{n-1}[X]\}\]
	
\end{enumerate}




\subsection*{Exercice 5}

\begin{enumerate} 
	\item $(a) \implies (b)$
	
	Si $f = 0$ ou $g = 0$ c'est évident, donc soit $f != 0$ et alors $g != 0$ par $(a)$.
	
	Comme $\dim \ker f + \dim \imag f = n$ et $\dim \imag f = 1$, puisque $\imag f \subset \R$ et $\imag f != \{0\}$, on a que $\dim \ker f = \dim \ker g = n-1$. Alors, on peut construire une base $B$ de $\R^n$ telle que $B = (u_1, \cdots, u_{n-1}, v_n)$ où $u_i \in \ker f$ et $f(v_n), g(v_n) \neq 0$. 
	
	D'ailleurs soit $x \in \R_n\ |\ x = \lambda_1 u_1 + \cdots + \lambda_n v_n$:
	
	\[\begin{cases}
		f(x) = \lambda_n f(v_n) \\
		g(x) = \lambda_n g(v_n)
	\end{cases}\] 
	
	\[\implies f(x) = \frac{f(v_n)}{g(v_n)} g(x)\]
	
	\item $(b) \implies (a)$
	
	Si $v \in \ker f$: $f(v) = \lambda g(v) = 0 \implies g(v) = 0 \implies v \in \ker g$, donc $\ker f \subset \ker g$. Si $u \in \ker g$: $g(u) = \dfrac{1}{\lambda} f(u) = 0\ \implies f(u) = 0 \implies u \in \ker f$, donc $\ker g \subset \ker f$.
	
	Alors $\ker f = \ker g$.
	
\end{enumerate}






\subsection*{Exercice 6}


\begin{enumerate}
	\item $P(x) = \det(M - x\cdot \id)$
	
	\[\det\begin{pmatrix}
		1 - x && 0 && 1 \\
		-1 && 2-x && 1 \\
		2-m && m-2 && m-x \\
	\end{pmatrix} = (1-x)(2-x)(m-x)\]
	
	Le polynôme minimal dépendra si $m = 1$ ou $m=2$ et si pour chaque un de ces valeurs la dimension de l'espace propre sera unique ou non, alors on verra après.
	
	
	\item Si $m \in \R \backslash \{1, 2\}$, la matrice est diagonalisable, puisqu'elle a 3 valeurs propres distincts. Maintenant pour:
	
	\begin{enumerate}
		\item $m = 1$
		
		\[A(1) = \begin{pmatrix}
			1 && 0 && 1 \\
			-1 && 2 && 1 \\
			1 && -1 && 1 \\
		\end{pmatrix}\]
		
		et, en calculant les vecteurs propres:
		
		\[Av = v = \begin{bmatrix}
			a \\
			b \\
			c \\
		\end{bmatrix}\]
		
		on arrive à la condition que $a = b$ et $c = 0$, de manière que $v = \lambda [1, 1, 0]$. Comme il n'y a que 1 dégrée de liberté, on arrive que $\dim E(1) < m(1) = $ la multiplicité polynômial de la racine. Donc il faut que $m \neq 1$.
		
		
		\item $m = 2$
		
		\[A(2) = \begin{pmatrix}
			1 && 0 && 1 \\
			-1 && 2 && 1 \\
			0 && 0 && 2 
		\end{pmatrix}\]
		
		En calculant les vecteurs propres on arrive que $a = c$ sans aucune restriction sur $b$, donc on a 2 dégrées de liberté pour choisir un vecteur propre: $v = \lambda_1 [1, 0, 1] + \lambda_2 [0, 1, 0]$. Donc même si $m = 2$ $A(m)$ est diagonalisable.
		
		
	\end{enumerate}
	
	De cette manière la condition cherchée est $m \in \R \backslash \{1\}$.
	
	
	\item Pour $m \neq 2$, les valeurs propres sont $1, 2, m$. Comme on a vu avant $v(\lambda = 1) = [1, 1, 0]$. Pour $v(\lambda = 2)$ on trouve qu'un vecteur propre associé est $[1, 0, 1]$. Pour $v(\lambda = m)$ on fait le même processus pour trouver une solution égale à $[1, 1, m-1]$:
	
	
	\[D = \diagonal{1 2 m}\]
	
	et 
	
	\[P = \begin{pmatrix}
		1 && 1 && 1 \\
		1 && 0 && 1 \\
		0 && 1 && m-1 \\
	\end{pmatrix}\]
	
	
	Pour $m = 2$, on va utiliser les résultats déjà montrés:
	
	\[D = \diagonal{1 2 2}\]
	
	et
	
	\[P = \begin{pmatrix}
		1 && 1 && 0 \\
		1 && 0 && 1 \\
		0 && 1 && 0 \\
	\end{pmatrix}\]
	
\end{enumerate}
	
	\subsection*{Exercice 7}
	
	En calculant son polynôme caractéristique
	
	\[\det (M(a) - x\cdot \id) = \begin{pmatrix}
		2 - x && 0 && 1 - a \\
		-1 && 1 - x && a - 1 \\
		a-1 && 0 && 2a - x 
	\end{pmatrix} = (1-x)(x - a - 1)^2\]
	
	Donc on a les valeurs propres $\lambda_1 = 1$ et $\lambda_2 = a + 1$, où $\lambda_2$ a multiplicité arithmétique $2$, donc on a besoin de déterminer si la multiplicité géométrique est aussi $2$, donc on cherchera les vecteurs $v = [x\ y\ z]\ | M(a)v = (a+1)v$:
	
	\[\begin{bmatrix}
		(a+1)x \\ (a+1)y \\ (a+1)z
	\end{bmatrix} = \begin{pmatrix}
	2 && 0 && 1-a \\
	-1 && 1 && a-1 \\
	a-1 && 0 && 2a
	\end{pmatrix}\begin{bmatrix}
	x \\ y \\z
	\end{bmatrix}\]
	
	On trouve que $(a-1)(x+z) = a(x+y) = 0$. Alors on va considérer les cas $a = 0, a = 1$ et $a \in \R\backslash\{0, 1\}$:
	
	\begin{enumerate}
		\item $a = 1$
		
		En ce cas $\lambda_1 \neq \lambda_2$ et on a que $x + y = 0$, de façon que le sous-espace propre de $\lambda_2$ a dimension\footnote{On peut choisir une base $B = ([1, -1, 0], [0, 0, 1])$ par exemple.} 2, qui, avec le sous-espace propre associé à $\lambda_1$, engendrent $\R^3$. \textbf{Donc la matrice est diagonalisable}.
		
		
		\item $a = 0$
		
		En ce cas $\lambda_1 = \lambda_2 = 1$, mais on la restriction pour les vecteurs propres: $x + z = 0$. Mais cette restriction permet un sous-espace de dimension 2 qui n'est pas capable d'engendrer $\R^3$. \textbf{Donc la matrice n'est pas diagonalisable}.
		
		
		\item $a \neq 0$ et $a \neq 1$
		
		En ce cas $\lambda_1 \neq \lambda_2$, et on a que $x + y = x + z = 0$. Mais cette restriction permet un sous-espace de dimension 1, qui même avec le sous-espace propre associé à $\lambda_1$ n'engendrent pas le $\R^3$. \textbf{Donc la matrice n'est pas diagonalisable}.
	\end{enumerate}
	
	
	Dans le cas $a = 1$:
	
	\[P = \begin{pmatrix}
		0 && 1 && 0\\
		1 && -1 &&0 \\
		0 && 0 && 1
	\end{pmatrix}\]
	
	et 
	
	\[D = \diagonal{1 2 2}\]
	
	
	\subsection*{Exercice 8}
	
	\begin{enumerate}
		\item D'abord on a que $B = (\varphi_0, \cdots, \varphi_n)$ et, comme $B$ a $n+1 = \dim E^*$ éléments, si l'on preuve que $B$ est linéairement indépendante, on montre qu'elle est une base de $E^*$.
		
		Alors, supposons que $\lambda_0 \varphi_0 + \cdots + \lambda_n \varphi_n = 0$, donc on a que:
		
		\[\left(\sum_{i=0}^n\lambda_i \varphi_i\right)(P) = 0\ (\forall P \in \R_n[X])\ \implies \sum_{i  = 0}^n \lambda_i P(a_i) = 0\ (\forall P \in \R_n[X])\] 
		
		Comme $a_i \neq a_j$ $\forall i \neq j$, on peut choisir des différents polynômes $Q_k(x)$ tels que $a_i$ sont racines de $Q_k$ sauf $a_k$ où $Q_k(a_k) = 1$:
		
		\[Q_k(x) = \alpha_k \cdot (x - a_0)\cdots(x- a_{k-1})(x-a_{k+1})\cdots (x-a_n)\]
		
		où $\alpha_k \in \R$ est la constant qui rend $Q_k(a_k) = 1$.
		
		De cette façon on montre que chaque $\lambda_k = 0$. Donc $B$ est base de $E^*$.
		
		\item Pour la définition de l'espace dual: 
		\[\phi_i(P_j) = P_j(a_i) = \delta_{i, j} = \begin{cases}
			1,\ \text{ si } i = j\\
			0,\ \text{ si } i \neq j
		\end{cases}\]
		
		On peut voir que les polynômes $Q_k(x)$ présentés dans l'item antérieur satisfont ces conditions. Pour montrer que $B = (Q_0, \cdots, Q_n)$ forme une base de $E$, il suffit de montrer que cet ensemble est linéairement indépendant, étant donné que $B$ contient $n+1 = \dim E$ éléments. Mais comme seulement $Q_i(x)$ n'est pas nul en $x = a_i$, il est claire que $B$ est L.I.		
		
	\end{enumerate}
	


\subsection*{Exercice 9}

\textbf{Partie I}

\begin{enumerate}
	\item Soit $w \in W$ tel que $w = \lambda_0v + \cdots + \lambda_{n-1}f^{n-1}(v)$. En appliquant $f$ sur $w$:
	
	\[\begin{split}f(w) &= \lambda_0 f(v) + \cdots + \lambda_{n-2}f^{n-1}(v) + \lambda_{n-1}f^n(v) \\
		&= \lambda_{n-1}a_0 v + (\lambda_{n-1}a_1 + \lambda_0) f(v) + \cdots + (\lambda_{n-1}a_{n-1} + \lambda_{n-2})f^{n-1}(v) \\
		\therefore f(w) &\in W\ \forall w \in W
	\end{split}\]
	
	\item On peut visualiser la matrice associée à cette application sur $W$ par rapport à la base $B = (v, f(v), \cdots, f^{n-1}(v))$:
	
	
	\[M(f_W) = \begin{pmatrix}
		0 && 0 && 0 && \cdots && a_0 \\
		1 && 0 && 0 && \cdots && a_1 \\
		0 && 1 && 0 && \cdots && a_2 \\
		\vdots && \vdots && \vdots && \ddots && \vdots \\
		0 && 0 && 0 && \cdots && a_{n-1}
	\end{pmatrix}\]
	
	Et, en sachant que $C_{f_W}(x) = \det(x \cdot \id_W-M(f_W))$:
	
	\[C_{f_W}(x) = \det \begin{bmatrix}
		x && 0 && 0 && \cdots && -a_0 \\
		-1 && x && 0 && \cdots && -a_1 \\
		0 && -1 && x && \cdots && -a_2 \\
		\vdots && \vdots && \vdots && \ddots && \vdots \\
		0 && 0 && 0 && \cdots && x-a_{n-1}
	\end{bmatrix}\]
	
	
	Pour calculer le déterminant, multipliez chaque ligne $i$ par $x^{i-1}$ et la sommez avec la première:
	
	\[C_{f_W}(x) = \det \begin{bmatrix}
		0 && 0 && 0 && \cdots && x^n - a_{n-1}x^{n-1} - \cdots - a_1 x - a_0 \\
		-1 && x && 0 && \cdots && -a_1 \\
		0 && -1 && x && \cdots && -a_2 \\
		\vdots && \vdots && \vdots && \ddots && \vdots \\
		0 && 0 && 0 && \cdots && x-a_{n-1}
	\end{bmatrix}\]
	
	Soit $P(x) = x^n - a_{n-1}x^{n-1} - \cdots - a_1 x - a_0$, on a que:
	
	\[C_{f_W}(x) = P(x) \det \begin{bmatrix}
		0 && 0 && 0 && \cdots && 1 \\
		-1 && x && 0 && \cdots && -a_1 \\
		0 && -1 && x && \cdots && -a_2 \\
		\vdots && \vdots && \vdots && \ddots && \vdots \\
		0 && 0 && 0 && \cdots && x-a_{n-1}
	\end{bmatrix} = P(x) (-1)^{n+1} \begin{bmatrix}
	-1 && x && \cdots && 0 \\
	0 && -1 && \cdots && 0 \\
	\vdots && \vdots && \ddots && \vdots \\
	0 && 0 && \cdots && -1
	\end{bmatrix}\]
	
	\[C_{f_W}(x) = P(x) (-1)^{n+1} (-1)^{n-1} = P(x)\]
	
	
	\item Comme $f_W$ est une restriction de $f$ elle contient la même propriété de $f$, telle que $f_W^n(v) = a_0v + a_1 f_W(v) + \cdots + a_{n-1}f_W^{n-1}(v)$.
	
	Maintenant, soit $u \in W$, on sait que $u = b_0 v + b_1f_W(v) + \cdots + b_{n-1}f_W^{n-1}(v)$, où $b_i \in \C$, de manière qu'il peut être représenté par: $u = Q(f_W)(v)$, où $Q(x) = b_0 + b_1x + \cdots + b_{n-1}x^{n-1}$. Alors, analysons:
	
	\[C_{f_W}(f_W) (u) = (C_{f_W}\cdot Q)(f_W(v)) = Q(f_W) \cdot \left[P(f_W)(v)\right]\]
	
	Mais comme $P(f_W)(v) = 0$, $C_{f_W}(f_W)(u) = 0\ \forall u \in W$, donc il faut que:
	
	\[C_{f_W}(f_W) = 0\]
	
	
	\item On peut construire une base de $V$ telle que $V = (v, f(v), \cdots, f^{n-1}(v), u_n, u_{n+1}, \cdots, u_d)$, de façon que la matrice associée à $f$ sera (en utilisant la notation de matrices par blocs):
	
	\[M(f) = \begin{pmatrix}
		M(f_W) && A \\
		0 && B \\
	\end{pmatrix}\]
	
	où $A \in M_{n, d-n}(\C)$ et $B \in M_{d-n, d-n}(\C)$. Alors, $C_f(x) = \det(x \id - M(f))$:
	
	\[C_f(x) = \det \begin{bmatrix}
		x \id - M(f_W) && -A \\
		0 && x \id - B 
	\end{bmatrix}\]
	
	Par les propriétés du déterminant de matrices par blocs, on arrive que:
	
	\[C_f(x) = \det(x \id - M(f_W)) \cdot \det(x \id - B) = C_{f_W}(x) \cdot \det(x \id - B)\]
	
	Donc on montre que $C_{f_W}(x)$ divise $C_f(x)$. 
\end{enumerate}

\textbf{Partie II}

On a montré que pour $v \in V\ | v \neq 0$: $C_{f_W}(f)(v) = 0$ ($C_{f_W}(f) = f^n - a_{n-1}f^{n-1} - \cdots - a_0\cdot \id$). Or, comme $C_{f_W}(x)$ divise $C_f(x)$, on a aussi que $C_f(f)(v) = 0$.

Donc comme $C_f(f)(v) = 0\ \forall v \in V$, il faut que:

\[C_f(f) = 0\ \QED\]




\subsection*{Exercice 10}

\begin{enumerate}
	\item Soit $v \in E$ avec le valeur propre $\lambda$ associé, on sait que par la propriété de la commutation: $f(g(v)) = g(f(v)) = g(\lambda v) = \lambda g(v)$. Donc on voit que $g(v) \in E$.
	
	
	\item Soit $\mu_w(x)$ le polynôme minimal de l'application linéaire $w$. Pour cet item on utilisera le fait que:
	
	\[\text{ les racines de } \mu_w \text{ ont multiplicité unitaire } \iff w \text{ est diagonalisable } \]
	
	Comme $g$ est diagonalisable, son polynôme minimal contient seulement racines simples (de multiplicités égales a 1). Par l'autre côté, comme le polynôme minimal de la restriction de $g$ en $E$ divise $\mu_g(x)$, il doit avoir racines simples aussi. Donc $g_E$ doit être diagonalisable.
	
	
	\item Comme $f$ est diagonalisable, sa matrice associé peut être écrite dans la forme suivante:
	
	\[M(f) = PD_fP^{-1} = P\begin{pmatrix}
		\Lambda_1 && 0 && \cdots && 0 \\
		0 && \Lambda_2 && \cdots && 0 \\
		\vdots && \vdots && \ddots && \vdots \\
		0 && 0 && \cdots && \Lambda_k \\
	\end{pmatrix}P^{-1}\]
	
	où $\Lambda_i \in M_{l_i, l_i}(K)$, ($K$ est le corps) telle que $l_i$ est la dimension du sous-espace propre associé au valeur propre $\lambda_i$ et:
	
	\[\Lambda_i = \diagonal{{\lambda_1} {\lambda_1} {\ddots} {\lambda_1}}\]
	
	
	De cette façon, comme les sous-espaces propres de $f$ sont stables sur $g$, on peut voir que la matrice associée à $g$, par rapport à la base $P$ est aussi diagonale par blocs:
	
	\[M(g) = P\begin{pmatrix}
		G_1 && 0 && \cdots && 0 \\
0 && G_2 && \cdots && 0 \\
\vdots && \vdots && \ddots && \vdots \\
0 && 0 && \cdots && G_k \\
	\end{pmatrix} P^{-1}\]
	
	où $G_i \in M_{l_i, l_i}(K)$. Mais, on a prouvé que chaque restriction $G_i$ est aussi diagonalisable, de façon que $G_i = P_i D_i P_i^{-1}$, où $D_i$ est diagonale. Donc voyez que l'on peut récrire $M(g)$:
	
	\[M(g) = P\begin{pmatrix}
		P_1 && 0 && \cdots && 0 \\
		0 && P_2 && \cdots && 0 \\
		\vdots && \vdots && \ddots && \vdots \\
		0 && 0 && \cdots && P_k \\
	\end{pmatrix}
	\begin{pmatrix}
		D_1 && 0 && \cdots && 0 \\
		0 && D_2 && \cdots && 0 \\
		\vdots && \vdots && \ddots && \vdots \\
		0 && 0 && \cdots && D_k \\
	\end{pmatrix}
	\begin{pmatrix}
	P_1^{-1} && 0 && \cdots && 0 \\
	0 && P_2^{-1} && \cdots && 0 \\
	\vdots && \vdots && \ddots && \vdots \\
	0 && 0 && \cdots && P_k^{-1} \\
	\end{pmatrix}P^{-1}\]
	
	En simplifiant:
	
	\[M(g) = PP_g\begin{pmatrix}
		D_1 && 0 && \cdots && 0 \\
		0 && D_2 && \cdots && 0 \\
		\vdots && \vdots && \ddots && \vdots \\
		0 && 0 && \cdots && D_k \\
	\end{pmatrix}
	P_g^{-1}P^{-1}\]
	
	Maintenant pour $M(f)$ e sa diagonale partie $D_f$, voyez que:
	
	\[P_gD_fP_g{-1} = \begin{pmatrix}
		P_1 && 0 && \cdots && 0 \\
		0 && P_2 && \cdots && 0 \\
		\vdots && \vdots && \ddots && \vdots \\
		0 && 0 && \cdots && P_k \\
	\end{pmatrix}
	\begin{pmatrix}
		\Lambda_1 && 0 && \cdots && 0 \\
		0 && \Lambda_2 && \cdots && 0 \\
		\vdots && \vdots && \ddots && \vdots \\
		0 && 0 && \cdots && \Lambda_k \\
	\end{pmatrix}
	\begin{pmatrix}
		P_1^{-1} && 0 && \cdots && 0 \\
		0 && P_2^{-1} && \cdots && 0 \\
		\vdots && \vdots && \ddots && \vdots \\
		0 && 0 && \cdots && P_k^{-1} \\
	\end{pmatrix}
\]

\[P_gD_fP_g{-1} = \begin{pmatrix}
	P_1\Lambda_1P_1^{-1} && 0 && \cdots && 0 \\
	0 && P_2\Lambda_2P_2^{-1} && \cdots && 0 \\
	\vdots && \vdots && \ddots && \vdots \\
	0 && 0 && \cdots && P_k\Lambda_kP_k^{-1} \\
\end{pmatrix} = \begin{pmatrix}
\Lambda_1 && 0 && \cdots && 0 \\
0 && \Lambda_2 && \cdots && 0 \\
\vdots && \vdots && \ddots && \vdots \\
0 && 0 && \cdots && \Lambda_k \\
\end{pmatrix} = D_f\]

Donc $M(f) = PP_g D_f P_g^{-1}P^{-1}$, où $PP_g$ est la matrice de cette base que diagonalise $f$ et $g$.


\item \textbf{La base de récurrence:} $\dim V = 1$: toutes les applications linéaires sont déjà diagonalisables.

\textbf{L'étape de récurrence:} Supposons que pour chaque sous-espace de dimension $\dim V < n$, le résultat est vrai. Considérons que $\dim V = n$.

Si toutes $f_i = \lambda_i \cdot \id_V\ |\ \lambda_i \in K$, le résultat est direct. Ainsi, de l'autre côté, s'il y a quelque $f_i \neq \lambda_i \cdot \id_V$, elle aura au moins deux valeurs propres distincts, de manière que ses espaces propres associés auront dimensions inférieures à $n$. Sans perte de généralité, considérons que $f_1$ ait cette propriété.

 Choisissez un sous-espace propre de $f_1$: $E_1$. Par la propriété de la commutation on conclut que chaque $f_j\ |\ j > 1$ est stable dans $E_1$:

\[f_j(E_1) \in E_1\]

Alors, comme $\dim E_1 < \dim V = n$, par l'hypothèse de récurrence on peut dire que chaque restriction de $f_j$ sur $E_1$: $f_{j, E_1}$ sont diagonalisables par la même base $B_1$. Rappelons que:

\[M(f_i) = PD_fP^{-1}\]

Si l'on répète ce processus pour chaque sous-espace propre de $f_1$, $E_1, E_2, \cdots, E_k$, ou trouve que pour chaque $f_i$:


\[M(f_i) = P\begin{pmatrix}
	F_{i,1} && 0 && \cdots && 0 \\
	0 && F_{i,2} && \cdots && 0 \\
	\vdots && \vdots && \ddots && \vdots \\
	0 && 0 && \cdots && F_{i,k} \\
\end{pmatrix} P^{-1}\]

Mais, on a montré que:

\[F_{i, j} = B_j D_{i, j} B_j^{-1}\]

Donc:

\[M(f_i) = P\begin{pmatrix}
	B_1 &&\cdots && 0 \\
	\vdots && \ddots && \vdots \\
	0 && \cdots && B_k \\
\end{pmatrix}
\begin{pmatrix}
	D_{i, 1} && \cdots && 0 \\
	\vdots && \ddots && \vdots \\
	0 && \cdots && D_{i, k} \\
\end{pmatrix}
\begin{pmatrix}
	B_1^{-1} && \cdots && 0 \\
	\vdots && \ddots && \vdots \\
	0 && \cdots && B_k^{-1} \\
\end{pmatrix}P^{-1}\]

\[M(f_i) = PB
\begin{pmatrix}
	D_{i, 1} && \cdots && 0 \\
	\vdots && \ddots && \vdots \\
	0 && \cdots && D_{i, k} \\
\end{pmatrix}B^{-1}P^{-1}\]

Et par le même raisonnement, $M(f_1)$ et aussi diagonalisable par rapport à la base $PB$. Donc on montre qu'elles sont co-diagonalisables.

\end{enumerate}


\subsection*{Exercice 11}

Voyons que:

\[A^2 - \id = 0\]

Donc $P(x) = x^2 - 1 = (x-1)(x+1)$ annihile $A$, et comme $P$ a des racines simples, $A$ doit être diagonalisable, avec les possibles vecteurs propres $\lambda \in \{1, -1\}$. De cette façon, pour $A, B \in G$, et en sachant que $G$ et un groupe:

\[A \cdot B \in G\]
\[\implies (AB)^2 = ABAB = I\]

Maintenant, multiplions chaque membre par $A$ à la gauche et $B$ à la droite:

\[A^2BAB^2 = AB\]
\[\therefore BA = AB\]

Donc chaque $A, B \in G$, commutent entre elles, de façon que l'on peut utiliser l'exercice antérieur: il y a une base $P$ telle que, chaque $A_i \in G$ est diagonalisable par $P$. Et, en plus, la matrice diagonale $D_i$ associée, aura les éléments $1$ ou $-1$ à la diagonale principal (car ils sont les seules valeurs propres possibles):

\[A_i = P \diagonal{{1 \text{ ou } -1} {1 \text{ ou } -1} \ddots {1 \text{ ou } -1}}P^{-1}\]

Donc on a seulement 2 choix possibles pour chaque éléments de la diagonale principal, de manière que le nombre maxime de $A_i$ distincts sera: $2^n$.




\subsection*{Exercice 12}

\begin{enumerate}
	\item $f$ est nilpotent $\iff$ $C_f(x) = x^n$.
	
	Par la définition d'un endomorphisme nilpotent, c'est claire que $C_f(x) = x^n\ \implies$ $f$ est nilpotent.
	
	Maintenant, considérons $f$ nilpotent, et $k$ le plus petit entier tel que $f^k = 0$. Ainsi, comme le polynôme minimal $\mu_f(x)$ divise $x^k$ mais ne peut pas avoir un dégrée inférieur à $k$, on conclure que $\mu_f(x) = x^k$, de façon que le seul valeur propre associé est $\lambda = 0$.
	
	Donc, comme les racines du polynôme caractéristique sont les mêmes de celles du minimal, il faut que $C_f(x) = x^n$.
	
	
	\item 
	
\begin{enumerate}
\item  Comme $f^{n-1} \neq 0$ il existe $v$ tel que $f^{n-1}(v) \neq 0$. Analysons:

\[\lambda_0 v + \lambda_1 f(v) + \cdots + \lambda_{n-1}f^{n-1}(v) = 0\]

Appliquez $f^{n-1}$ à cette expression. Comme $f^k(v) = 0,\ \forall k \ge n$, il ne restera que:

\[\lambda_0 f^{n-1}(v) = 0\]
\[\implies \lambda_0 = 0\]

De la même façon on peut conclure que chaque $\lambda_i$ doit être zéro. Ainsi, comme l'ensemble a $n$ éléments linéairement indépendants, il est une base $B$ de $V$.


\item

\[M_B(f) = 	\begin{pmatrix}
	0 && 0 && \cdots && 0 \\
	1 && 0 && \cdots && 0 \\
	0 && 1 && \cdots && 0 \\
	\vdots && \vdots && \ddots && \vdots \\
	0 && 0 && \cdots && 0
\end{pmatrix}\]

On peut voir que $M_B(f)$ est déjà une matrice échelonnée inférieurement (triangulaire), donc son rang est le nombre de colonnes non nulles: $n-1$ (seulement la dernière est nulle).


\item Si $g = P(f)$, c'est direct que $g$ commute à $f$.

Or, si $g$ commute à $f$:

\[g \circ f = f \circ g\]
\[(g \circ f)(f^{k-1}) = (f \circ g)(f^{k-1})\]

Et, comme $f \circ g \circ f^{k-1} = g \circ f^{k}$:

\[g \circ f^k = f^k \circ g\]

Pour un $u \in V$ on a que $u = Q[f](v)$, et supposons que $g(v) = P[f](v)$, où $Q(x), P(x)$ sont des polynômes de degré maxime $(n-1)$:

\[g(u) = g\left(Q[f](v)\right) = Q[f]P[f](v) = P[f]Q[f](v) = P[f](u)\]

Donc $g = P[f]$.
\end{enumerate}


\item Par définition, $f_0(v) = f(v) \subset V_0$. On voit aussi que, comme $f_0^n (v) = f^n(v) = 0$, $f_0$ est aussi nilpotent. De cette façon:

\[\ker f_0 = \ker f \cap \imag f\]

où, comme on a défini dans l'item antérieur:

\[\imag f = \langle f(v), f^2(v), \cdots, f^{n-1}(v)\rangle\]

Or, comme $f^{n-1}(v) \in \ker f$ et  $\dim \ker f = 1$, vu que $\dim \imag f = n-1$ (théorème du rang):

\[\ker f = \langle f^{n-1}(v) \rangle\]

D'ailleurs, $f^{n-1}(v) = f(f^{n-2}(v))$, alors $\langle f^{n-1}(v) \rangle = \ker f \subset \imag(f) = V_0$, donc:

\[\ker(f_0) = \ker f \cap \imag f = \ker f\]

Finalement, par le théorème du rang:

\[\dim \imag f_0 = n-1 - \dim \ker f_0 = n-2\]

\item Voyez que nous avons déjà prouvé $(a) \implies (b)$ et $(b) \implies (c)$, alors on montrera seulement l'implication $(c) \implies (a)$:

Pour $n = 1$ le résultat est direct, étant donné que $f^{1-1} = \id$.

Pour $n > 1$: si $\dim \imag f = n - 1$, on a que $\dim \ker f = 1$. Comme on a montrer pour l'exercice 3:

\[\dim \imag f^k - \dim \imag f^{k+1} = \dim \left(\ker f \cap \imag f^k\right) \le 1\]

Maintenant sommons cette expression pour chaque $k \in \N\ |\ 0 \le k \le n-2$, (donc pour $n-1$ valeurs de $k$):

\[\dim V - \dim \imag f^{n-1} \le n-1\]
\[\dim \imag f^{n-1} \ge 1\]

Donc il faut que $f^{n-1} \neq 0$.
\end{enumerate}


\subsection*{Exercice 13}

\begin{enumerate}
\item \textbf{Stabilité de la Somme et du Produit}

Soient:

\[A = \begin{pmatrix}
	x && -y \\
	\bar{y} && \bar{x} 
\end{pmatrix}\]

\[B = \begin{pmatrix}
	a && -b \\
	\bar{b} && \bar{a} 
\end{pmatrix}\]

Alors:

\[A+B = \begin{pmatrix}
	x+a && -y-b \\
	\bar{y} + \bar{b} && \bar{x} + \bar{a} 
\end{pmatrix} = \begin{pmatrix}
x+a && -(y+b) \\
\overline{y + b} && \overline{x + a} 
\end{pmatrix} \in \mathbb{H}\]


Par ailleurs:

\[A\cdot B = \begin{pmatrix}
	ax - \bar{b}y && -\bar{a}y - bx \\
	a\bar{y} + \bar{b}\bar{x} && \bar{a}\bar{x} - b\bar{y}
\end{pmatrix} = \begin{pmatrix}
	ax - \bar{b}y && -(\bar{a}y + bx) \\
	\overline{\bar{a}y + bx} && \overline{ax - \bar{b}y}
\end{pmatrix} \in \mathbb{H}\]


\item \textbf{Stabilité par multiplication par un scalaire}

 Pour un $\lambda \in \R$ il suit que $\lambda = \bar{\lambda}$, de façon que:
 
 \[\lambda A = \begin{pmatrix}
 	\lambda x && -\lambda y \\
 	\lambda \bar{y} && \lambda \bar{x} 
 \end{pmatrix} = \begin{pmatrix}
 \lambda x && -\lambda y \\
 \overline{\lambda y} && \overline{\lambda x}
 \end{pmatrix} \in \mathbb{H}\]
 
 
 Donc, comme cet ensemble possède les propriétés de la somme, produit par scalaire, et aussi l'élément neutre:
 
 \[0 = \begin{pmatrix}
 	0 && 0 \\ 0 && 0 
 \end{pmatrix}\]
 
 On peut conclure que $\mathbb{H}$ est un espace vectoriel réel (puisque la propriété du scalaire n'est valide que pour $\lambda \in \R$).
 
 
 \item \textbf{Vérification d'une base} 
 
 Notez qu'une matrice générique $A \in \mathbb{H}$  avec $x = a+ib$ et $y=c+id$ ($a, b, c, d \in \R$), peut être exprimée de la façon suivante:
 
 \[A = \begin{pmatrix}
 	a + ib && -c-id \\
 	c - id && a-ib 
 \end{pmatrix} = a \id + b \cdot u + c \cdot v + d \cdot w\]
 
 Alors $\langle \id, u, v, w\rangle$ engendre $\mathbb{H}$ et on voit que $A = 0 \implies a=b=c=d=0$, de manière que cet ensemble est linéairement indépendant. Donc il est une base de $\mathbb{H}$.
 
 \item \textbf{Polynôme caractéristique et minimal}
 
 Le polynôme caractéristique:
 
 \[C_A(\lambda) = \det \begin{bmatrix}
 	x - \lambda && -y \\
 	\bar{y} && \bar{x} -\lambda
 \end{bmatrix} = \lambda^2 - (x+\bar{x})\lambda + x\bar{x} + y\bar{y} = \lambda^2 - 2 \text{Re}(x)\lambda + |x|^2 + |y|^2\]
 
 Comme $|x|^2 + |y|^2 > \text{Re}^2(x)$, on conclut que $C_A(\lambda)$ n'a pas des solutions réels, alors les racines sont complexes et distincts entre elles, de façon que le polynôme minimal sera identique à $C_A(\lambda)$.
 
 
 \item \textbf{Matrice Inverse}
 
 
 Si $A \neq 0$, $|x|^2 + |y|^2 \neq 0$. Ainsi, par le polynôme caractéristique:
 
 \[(|x|^2 + |y|^2)\id = (2 \text{Re}(x)\cdot\id - A)A\]
 
 Donc on voit que:
 
 \[A^{-1} = \frac{1}{|x|^2 + |y|^2}(2 \text{Re}(x)\cdot \id - A)\]
 
 \[\therefore A^{-1} = \frac{1}{|x|^2 + |y|^2}\begin{pmatrix}
 	\bar{x} && y \\
 	-\bar{y} && x 
 \end{pmatrix}\]
 
 
 Alors on voit que cet ensemble accomplit les conditions pour être considéré un corps.
 
\end{enumerate}



\subsection*{Exercice 14}

\begin{enumerate}
	
\item Soit $P(x)$ le polynôme minimal de $A$. On voit que:

\[P[\varphi](M) = P[A](M)\]

Donc si $P(x)$ annihile $A$, il annihilera aussi $\varphi$, et comme $P(x)$ a des racines simples (puisque $A$ est diagonalisable), le polynôme minimal de $\varphi$ aura aussi des racines simples, donc $\varphi$ est diagonalisable. 

\item Par l'hypothèse:

\[\varphi(M_0) = AM_0 = \lambda_\varphi M_0\]
\[AX_0 = \lambda_A X_0\]
\[\implies AM_0X_0 = \lambda_\varphi M_0X_0\]

Donc $M_0X_0$ est vecteur propre de $A$ s'il n'est pas nul. En fait, ce même raisonnement est valide si $X_0$ n'était pas un vecteur propre.


\item Comme $X = [x_1, x_2, \cdots, x_n]^T \neq 0$, il y a au moins un valeur $x_i$ tel que $x_i \neq 0$. Donc on peut construire $B$ telle que:

\[B = Y\cdot \left(\frac{1}{x_i}e_i^T\right)\]

où $e_i$ est l'élément $i$ de la base canonique du $\R^n$. Par exemple: $e_2 = [0, 1, 0, \cdots, 0]^T$. Cela fonctionne car $e_i^T\cdot X = x_i$.


Ensuite, comme $B \in M_n(\R)$, elle peut être représentée comme une combinaison linéaire de la base $M_i$:

\[B = \sum_{i=1}^{n^2} \lambda_i M_i\]
\[\therefore Y = \sum_{i=1}^{n^2} \lambda_i (M_iX)\ \forall Y \in \R^n\]

Donc $\langle M_1X, \cdots, M_{n^2}X \rangle$ engendre $\R^n$.

\item On pourrait utiliser le même argument du polynôme caractéristique, mais cela serait une gaspillage du développement que l'on a fait jusqu'à maintenant.

Si $\varphi$ est diagonalisable, on peut choisir $M_i$ (les éléments de la base de $M_n(\R)$) vecteurs propres de $\varphi$. De cette manière, par l'item (2), on sait que $M_iX$ est aussi un valeur propre de $A$.

Donc on a que l'ensemble de vecteurs propres de $A$: $B_A = \langle M_1X, \cdots, M_{n^2}X \rangle$ a $n^2$ éléments et engendre $\R^n$.

Initialement considérons $E_A = \{\}$ un ensemble vide. On peut itérer pour chaque $v_i = M_iX$ et l'ajouter à l'ensemble $E_A$ si $v_i$ n'est pas déjà une combinaison linéaire des éléments de $E_A$. À la fin, on aura que $E_A$ est composé de vecteurs propres de $A$, il engendre $\R^n$ et il est linéairement indépendant, donc $A$ est diagonalisable. $\QED$

\vspace{5cm}
	
\end{enumerate}
